{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import algorithms.heartrate as hr\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow settings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data_train, y_data_train, groups_train = ...\n",
    "# x_data_test, y_data_test, groups_test = ...\n",
    "\n",
    "# dummy:\n",
    "train_size, test_size = 10000, 3000\n",
    "n_groups_train, n_groups_test = 28, 14\n",
    "x_data_train = np.random.normal(size=(train_size, 400, 1))\n",
    "y_data_train = np.random.normal(loc=68, scale=10., size=(train_size,))\n",
    "groups_train = np.sort(np.random.randint(n_groups_train, size=train_size))\n",
    "\n",
    "x_data_test = np.random.normal(size=(test_size, 400, 1))\n",
    "y_data_test = np.random.normal(loc=68, scale=10., size=(test_size,))\n",
    "groups_test = np.sort(np.random.randint(n_groups_test, size=test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelnames = [\n",
    "    \"models.deeper_fcn-x1\",\n",
    "    \"models.stacked_cnn_rnn_improved-x1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dl_train = {\"true\": y_data_train, \"groups\": groups_train}\n",
    "predictions_dl_test = {\"true\": y_data_test, \"groups\": groups_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modelpath(modelname):\n",
    "    return os.path.join(\"output\", modelname)  # modify this if necessary\n",
    "\n",
    "def get_predictions(modelname, weights_format=\"weights-{:02d}.h5\",\n",
    "                    batch_size=32, train=True):\n",
    "    modelpath = get_modelpath(modelname)\n",
    "    model = utils.get_model_from_json(modelpath)\n",
    "    \n",
    "    if not train:\n",
    "        model.load_weights(os.path.join(modelpath, \"final\",\n",
    "                                        weights_format.format(0)))\n",
    "        y_pred = model.predict(x_data_test)\n",
    "        tf.keras.backend.clear_session()\n",
    "        return y_pred[:, 0]\n",
    "\n",
    "    splitter = utils.get_group_splitter(n_groups_train, groups_train)\n",
    "    results = []\n",
    "    for i, (_, v_inds) in enumerate(splitter):\n",
    "        model.load_weights(os.path.join(modelpath, weights_format.format(i)))\n",
    "        y_pred = model.predict(x_data_train[v_inds], batch_size=batch_size)\n",
    "        results = np.r_[results, y_pred[:, 0]]  # append new predictions\n",
    "    tf.keras.backend.clear_session()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 400, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 196, 64)           640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 196, 64)           256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 196, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 96, 32)            10272     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 96, 32)            128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 96, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 46, 32)            128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 46, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 21, 32)            5152      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 21, 32)            128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (None, 21, 32)            6336      \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (None, 21, 16)            2400      \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 21, 8)             624       \n",
      "_________________________________________________________________\n",
      "cu_dnngru_3 (CuDNNGRU)       (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 31,389\n",
      "Trainable params: 31,069\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "utils.get_model_from_json(get_modelpath(modelnames[-1])).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models.deeper_fcn-x1 8.32351764234299\n",
      "models.stacked_cnn_rnn_improved-x1 8.364179138413347\n"
     ]
    }
   ],
   "source": [
    "for modelname in modelnames:\n",
    "    predictions_dl_train[modelname] = get_predictions(modelname)\n",
    "    predictions_dl_test[modelname] = get_predictions(modelname, train=False)\n",
    "    print(modelname, hr.hr_mae(y_data_test, predictions_dl_test[modelname]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on testing data:\n",
      "========================\n",
      "\n",
      " modelname                          |   mae |   std\n",
      "------------------------------------+-------+-------\n",
      " models.stacked_cnn_rnn_improved-x1 | 8.317 | 0.362\n",
      " models.stacked_cnn_rnn_improved-x1 | 8.361 | 0.329\n"
     ]
    }
   ],
   "source": [
    "# calculate MAE and MAPE patient-wise (for training and testing set)\n",
    "dl_results_train = np.zeros((len(modelnames), 1, 2, n_groups_train))\n",
    "dl_results_test = np.zeros((len(modelnames), 1, 2, n_groups_test))\n",
    "\n",
    "for res, preds_ in [(dl_results_train, predictions_dl_train),\n",
    "                    (dl_results_test, predictions_dl_test)]:\n",
    "    y_true, preds = preds_[\"true\"], preds_\n",
    "    for ni, name in enumerate(modelnames):\n",
    "        y_pred = preds[name]\n",
    "        for gi, group in enumerate(np.unique(preds[\"groups\"])):\n",
    "            yt = y_true[preds[\"groups\"] == group]\n",
    "            yp = y_pred[preds[\"groups\"] == group]\n",
    "            res[ni, 0, :, gi] = (hr.hr_mape(yt, yp), hr.hr_mae(yt, yp))\n",
    "\n",
    "score_index = 1  # 0: MAPE, 1: MAE\n",
    "rows = []\n",
    "for ni, name in enumerate(modelnames):\n",
    "    errmean = dl_results_test.mean(axis=-1)[ni, 0, score_index]\n",
    "    errstd = dl_results_test.std(axis=-1)[ni, 0, score_index]\n",
    "    rows.append((modelname, errmean, errstd))\n",
    "\n",
    "print(\"Results on testing data:\")\n",
    "print(\"========================\\n\")\n",
    "print(tabulate(rows, tablefmt=\"presto\", floatfmt=\".3f\",\n",
    "               headers=[\"modelname\", \"mae\" if score_index==1 else \"mape\", \"std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
