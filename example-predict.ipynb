{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 00:16:43.250713: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import tensorflow._api.v2.compat.v1 as tf\n",
    "\n",
    "\n",
    "import algorithms.heartrate as hr\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tensorflow settings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# x_data_train, y_data_train, groups_train = ...\n",
    "# x_data_test, y_data_test, groups_test = ...\n",
    "\n",
    "# dummy:\n",
    "train_size, test_size = 774, 258\n",
    "n_groups_train, n_groups_test = 27, 9\n",
    "data = np.genfromtxt('data/experiments/sleeprawlive.csv', delimiter=';')\n",
    "data = data[:len(data)-len(data)%400]\n",
    "data = data.reshape( int(len(data)/400), 400)\n",
    "x_data_train = np.expand_dims(data, axis=2)\n",
    "data = np.genfromtxt('data/experiments/gold.csv', delimiter=',')\n",
    "data = data[:len(data)-len(data)%16]\n",
    "y_data_train = np.mean(data.reshape(int(len(data)/16), 16), axis=1)\n",
    "y_data_train = y_data_train[:774]\n",
    "groups_train = 27\n",
    "\n",
    "x_data_test = x_data_train[200:458]\n",
    "y_data_test = y_data_train[200:458]\n",
    "groups_test = 9"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "modelnames = [\n",
    "    \"models.deeper_fcn-x1\",\n",
    "    #\"models.stacked_cnn_rnn_improved-x1\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions_dl_train = {\"true\": y_data_train, \"groups\": groups_train}\n",
    "predictions_dl_test = {\"true\": y_data_test, \"groups\": groups_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_modelpath(modelname):\n",
    "    return os.path.join(\"output\", modelname)  # modify this if necessary\n",
    "\n",
    "def get_predictions(modelname, weights_format=\"weights-{:02d}.h5\",\n",
    "                    batch_size=32, train=False):\n",
    "    modelpath = get_modelpath(modelname)\n",
    "    model = utils.get_model_from_json(modelpath)\n",
    "    \n",
    "    if not train:\n",
    "        model.load_weights(os.path.join(modelpath,\n",
    "                                        weights_format.format(0)))\n",
    "        y_pred = model.predict(x_data_test)\n",
    "        tf.keras.backend.clear_session()\n",
    "        return y_pred[:, 0]\n",
    "\n",
    "    splitter = utils.get_group_splitter(n_groups_train, groups_train)\n",
    "    results = []\n",
    "    for i, (_, v_inds) in enumerate(splitter):\n",
    "        model.load_weights(os.path.join(modelpath, weights_format.format(i)))\n",
    "        y_pred = model.predict(x_data_train[v_inds], batch_size=batch_size)\n",
    "        results = np.r_[results, y_pred[:, 0]]  # append new predictions\n",
    "    tf.keras.backend.clear_session()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 400, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_132 (Conv1D)         (None, 390, 8)            96        \n",
      "                                                                 \n",
      " batch_normalization_96 (Bat  (None, 390, 8)           32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_120 (LeakyReLU)  (None, 390, 8)           0         \n",
      "                                                                 \n",
      " conv1d_133 (Conv1D)         (None, 190, 8)            712       \n",
      "                                                                 \n",
      " batch_normalization_97 (Bat  (None, 190, 8)           32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_121 (LeakyReLU)  (None, 190, 8)           0         \n",
      "                                                                 \n",
      " conv1d_134 (Conv1D)         (None, 180, 16)           1424      \n",
      "                                                                 \n",
      " batch_normalization_98 (Bat  (None, 180, 16)          64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_122 (LeakyReLU)  (None, 180, 16)          0         \n",
      "                                                                 \n",
      " conv1d_135 (Conv1D)         (None, 86, 16)            2320      \n",
      "                                                                 \n",
      " batch_normalization_99 (Bat  (None, 86, 16)           64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_123 (LeakyReLU)  (None, 86, 16)           0         \n",
      "                                                                 \n",
      " conv1d_136 (Conv1D)         (None, 78, 16)            2320      \n",
      "                                                                 \n",
      " batch_normalization_100 (Ba  (None, 78, 16)           64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_124 (LeakyReLU)  (None, 78, 16)           0         \n",
      "                                                                 \n",
      " conv1d_137 (Conv1D)         (None, 36, 16)            1808      \n",
      "                                                                 \n",
      " batch_normalization_101 (Ba  (None, 36, 16)           64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_125 (LeakyReLU)  (None, 36, 16)           0         \n",
      "                                                                 \n",
      " conv1d_138 (Conv1D)         (None, 28, 16)            2320      \n",
      "                                                                 \n",
      " batch_normalization_102 (Ba  (None, 28, 16)           64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_126 (LeakyReLU)  (None, 28, 16)           0         \n",
      "                                                                 \n",
      " conv1d_139 (Conv1D)         (None, 11, 16)            1808      \n",
      "                                                                 \n",
      " batch_normalization_103 (Ba  (None, 11, 16)           64        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_127 (LeakyReLU)  (None, 11, 16)           0         \n",
      "                                                                 \n",
      " conv1d_140 (Conv1D)         (None, 5, 8)              904       \n",
      "                                                                 \n",
      " leaky_re_lu_128 (LeakyReLU)  (None, 5, 8)             0         \n",
      "                                                                 \n",
      " conv1d_141 (Conv1D)         (None, 1, 8)              328       \n",
      "                                                                 \n",
      " leaky_re_lu_129 (LeakyReLU)  (None, 1, 8)             0         \n",
      "                                                                 \n",
      " conv1d_142 (Conv1D)         (None, 1, 1)              9         \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 1)                 0         \n",
      "                                                                 \n",
      " lambda_12 (Lambda)          (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,497\n",
      "Trainable params: 14,273\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "utils.get_model_from_json(get_modelpath(modelnames[-1])).summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true': array([28.4375, 17.5625, 46.25  , 49.9375, 26.5625, 18.875 , 30.6875,\n",
      "       63.9375, 32.5625, 23.5   , 20.1875, 78.375 , 56.625 , 35.375 ,\n",
      "       19.8125, 42.3125, 55.875 , 25.5   , 15.0625, 18.375 , 56.4375,\n",
      "       28.0625, 18.75  , 12.5625, 59.5625, 48.0625, 27.8125, 16.25  ,\n",
      "       47.9375, 73.5625, 36.25  , 23.25  , 25.9375, 73.5625, 39.375 ,\n",
      "       28.    , 23.625 , 80.0625, 62.125 , 42.6875, 30.6875, 63.0625,\n",
      "       74.4375, 46.9375, 35.125 , 42.375 , 79.4375, 43.875 , 29.5625,\n",
      "       23.    , 65.6875, 42.1875, 28.5   , 19.125 , 61.4375, 57.625 ,\n",
      "       34.375 , 21.9375, 58.9375, 77.6875, 48.4375, 36.375 , 44.5   ,\n",
      "       84.8125, 49.6875, 36.75  , 29.9375, 68.8125, 47.5625, 34.0625,\n",
      "       24.0625, 55.5625, 55.8125, 35.4375, 24.625 , 48.4375, 80.5625,\n",
      "       46.4375, 33.125 , 32.1875, 80.3125, 49.375 , 35.0625, 25.0625,\n",
      "       63.375 , 55.25  , 37.625 , 26.125 , 55.3125, 67.5   , 39.375 ,\n",
      "       25.1875, 50.75  , 82.6875, 51.1875, 38.875 , 41.5   , 85.4375,\n",
      "       52.5625, 38.625 , 30.6875, 70.875 , 50.125 , 35.5   , 25.6875,\n",
      "       70.5625, 67.5   , 42.9375, 29.9375, 60.4375, 79.    , 49.5625,\n",
      "       37.9375, 42.0625, 75.0625, 46.9375, 35.875 , 32.75  , 70.6875,\n",
      "       51.9375, 37.8125, 30.75  , 83.875 , 61.0625, 42.8125, 32.875 ,\n",
      "       79.4375, 61.8125, 41.125 , 30.625 , 77.1875, 68.0625, 45.625 ,\n",
      "       34.6875, 65.6875, 74.375 , 46.1875, 34.875 , 44.8125, 70.125 ,\n",
      "       41.4375, 31.3125, 30.5625, 69.5   , 44.875 , 34.9375, 28.25  ,\n",
      "       60.75  , 46.75  , 31.875 , 22.75  , 63.6875, 61.0625, 34.5   ,\n",
      "       22.6875, 53.0625, 77.9375, 44.4375, 32.75  , 36.875 , 81.5   ,\n",
      "       46.9375, 33.5   , 24.125 , 54.9375, 42.9375, 29.6875, 21.0625,\n",
      "       44.9375, 50.6875, 32.1875, 20.    , 46.75  , 72.0625, 33.5625,\n",
      "       17.8125,  9.875 , 15.25  ,  5.0625,  5.    ,  6.9375, 16.25  ,\n",
      "        9.5625,  8.6875, 12.5   , 32.125 , 23.5625, 19.5625, 24.5625,\n",
      "       84.4375, 62.125 , 49.125 , 54.1875, 92.9375, 68.0625, 56.    ,\n",
      "       52.875 , 87.75  , 62.5   , 49.875 , 49.125 , 86.8125, 57.5625,\n",
      "       44.9375, 53.75  , 87.8125, 56.6875, 45.375 , 63.8125, 84.75  ,\n",
      "       55.3125, 44.1875, 73.5   , 79.125 , 52.1875, 43.375 , 82.75  ,\n",
      "       70.5625, 47.3125, 41.125 , 85.625 , 63.5   , 42.5   , 34.75  ,\n",
      "       82.625 , 63.6875, 41.625 , 30.875 , 72.4375, 71.5625, 43.125 ,\n",
      "       31.    , 61.75  , 76.1875, 42.    , 30.125 , 54.25  , 78.9375,\n",
      "       41.25  , 29.0625, 38.375 , 85.4375, 45.625 , 32.125 , 25.25  ,\n",
      "       79.125 , 57.0625, 35.5   , 24.1875, 56.9375, 70.4375, 36.6875,\n",
      "       25.8125, 44.625 , 83.5625, 44.625 , 33.    , 32.1875, 87.0625,\n",
      "       55.625 , 38.625 , 28.4375, 69.625 , 67.25  , 39.3125, 28.    ,\n",
      "       39.9375, 81.5625, 41.9375, 30.75  , 23.75  , 79.8125, 63.125 ,\n",
      "       40.5625, 29.5   , 45.0625, 84.25  , 46.375 , 34.3125, 24.75  ,\n",
      "       48.875 , 73.875 , 39.5625, 29.8125, 28.9375, 80.    , 46.75  ,\n",
      "       33.    , 23.0625, 63.5   , 74.1875, 43.75  , 33.1875, 29.625 ,\n",
      "       82.5   , 56.875 , 38.875 , 28.3125, 42.9375, 79.375 , 42.25  ,\n",
      "       31.625 , 24.125 , 70.875 , 59.875 , 36.75  , 25.8125, 38.375 ,\n",
      "       86.5   , 49.    , 37.0625, 27.6875, 65.9375, 70.625 , 42.25  ,\n",
      "       31.625 , 30.0625, 81.75  , 50.625 , 35.1875, 25.25  , 48.375 ,\n",
      "       76.4375, 40.5625, 29.5   , 25.875 , 84.5   , 59.0625, 41.25  ,\n",
      "       30.875 , 49.0625, 83.0625, 47.4375, 36.375 , 28.    , 70.25  ,\n",
      "       65.5625, 40.625 , 29.625 , 33.375 , 84.25  , 47.6875, 34.9375,\n",
      "       24.875 , 67.8125, 72.5625, 44.4375, 33.75  , 34.1875, 86.125 ,\n",
      "       54.625 , 39.375 , 29.0625, 52.8125, 76.4375, 42.5625, 32.    ,\n",
      "       27.125 , 78.25  , 54.3125, 36.75  , 25.75  , 54.625 , 80.375 ,\n",
      "       46.875 , 35.9375, 31.    , 82.5625, 59.75  , 40.9375, 30.25  ,\n",
      "       46.375 , 78.875 , 43.375 , 32.875 , 26.1875, 75.625 , 56.1875,\n",
      "       37.5625, 26.4375, 55.6875, 79.25  , 45.9375, 35.375 , 35.1875,\n",
      "       87.9375, 58.4375, 42.125 , 31.5   , 60.75  , 76.75  , 44.75  ,\n",
      "       33.8125, 29.8125, 79.75  , 52.625 , 37.0625, 26.5   , 54.5   ,\n",
      "       78.375 , 44.25  , 33.375 , 29.5   , 84.8125, 60.3125, 41.5   ,\n",
      "       30.8125, 49.375 , 82.25  , 45.75  , 34.4375, 26.9375, 73.6875,\n",
      "       57.9375, 37.3125, 26.5   , 41.25  , 82.625 , 43.9375, 32.8125,\n",
      "       25.5   , 80.4375, 64.75  , 43.0625, 32.375 , 45.125 , 85.625 ,\n",
      "       49.25  , 37.25  , 27.8125, 66.375 , 69.8125, 41.5625, 30.625 ,\n",
      "       29.3125, 83.625 , 52.375 , 36.75  , 25.8125, 49.5   , 81.6875,\n",
      "       45.9375, 34.875 , 26.3125, 67.0625, 70.3125, 42.375 , 31.6875,\n",
      "       27.125 , 78.5   , 57.5   , 38.375 , 27.375 , 33.5   , 83.875 ,\n",
      "       46.125 , 33.8125, 23.6875, 58.5   , 75.    , 43.1875, 33.    ,\n",
      "       27.5   , 79.875 , 60.875 , 41.375 , 30.8125, 36.6875, 84.4375,\n",
      "       48.    , 35.8125, 26.125 , 57.4375, 72.75  , 41.1875, 30.5   ,\n",
      "       29.25  , 86.25  , 55.5   , 40.3125, 29.8125, 52.1875, 80.625 ,\n",
      "       46.125 , 35.75  , 28.0625, 71.1875, 64.5   , 40.4375, 30.    ,\n",
      "       31.    , 82.5625, 48.5   , 35.5625, 25.25  , 60.5625, 75.25  ,\n",
      "       44.625 , 34.4375, 29.375 , 81.1875, 61.5   , 42.4375, 32.1875,\n",
      "       38.125 , 85.25  , 49.125 , 36.8125, 27.0625, 52.4375, 72.    ,\n",
      "       40.25  , 30.375 , 25.25  , 76.1875, 55.625 , 37.4375, 26.375 ,\n",
      "       49.6875, 83.25  , 49.125 , 38.75  , 31.6875, 77.875 , 63.75  ,\n",
      "       42.6875, 32.5   , 39.9375, 77.8125, 43.5625, 33.8125, 26.375 ,\n",
      "       70.3125, 60.5625, 39.0625, 28.6875, 56.125 , 82.1875, 49.875 ,\n",
      "       39.5625, 36.8125, 85.8125, 58.6875, 42.25  , 32.1875, 56.0625,\n",
      "       72.375 , 41.9375, 32.3125, 31.0625, 77.3125, 47.    , 35.3125,\n",
      "       26.8125, 73.    , 68.1875, 43.375 , 33.125 , 45.    , 86.375 ,\n",
      "       50.375 , 38.3125, 30.0625, 71.3125, 61.3125, 39.    , 29.25  ,\n",
      "       39.    , 76.125 , 41.625 , 32.125 , 26.875 , 80.75  , 59.1875,\n",
      "       40.1875, 29.8125, 52.8125, 81.    , 46.3125, 35.625 , 30.125 ,\n",
      "       78.5625, 56.125 , 38.0625, 27.75  , 45.5625, 73.8125, 40.0625,\n",
      "       30.5625, 28.0625, 83.875 , 53.9375, 38.3125, 28.0625, 59.6875,\n",
      "       76.625 , 45.375 , 35.3125, 33.375 , 84.5   , 54.125 , 38.875 ,\n",
      "       28.9375, 50.625 , 75.1875, 41.4375, 31.4375, 27.1875, 78.125 ,\n",
      "       52.1875, 36.125 , 25.9375, 62.875 , 74.6875, 45.    , 35.    ,\n",
      "       37.    , 86.625 , 52.875 , 39.5625, 30.1875, 63.6875, 69.6875,\n",
      "       41.375 , 31.0625, 34.625 , 82.    , 46.0625, 34.5   , 25.875 ,\n",
      "       74.875 , 66.1875, 42.875 , 32.8125, 43.75  , 85.25  , 48.5   ,\n",
      "       37.375 , 28.875 , 69.6875, 61.9375, 39.    , 29.125 , 37.3125,\n",
      "       79.6875, 43.1875, 33.0625, 26.5625, 80.4375, 61.625 , 42.0625,\n",
      "       31.875 , 52.    , 82.25  , 47.25  , 36.875 , 31.25  , 76.375 ,\n",
      "       54.9375, 38.1875, 28.5   , 49.125 , 75.875 , 41.875 , 32.    ,\n",
      "       31.75  , 87.1875, 55.5625, 41.    , 31.1875, 64.1875, 73.6875,\n",
      "       43.875 , 33.625 , 33.3125, 79.5   , 47.125 , 35.125 , 26.    ,\n",
      "       61.8125, 65.375 , 39.125 , 29.0625, 47.9375, 85.    , 50.    ,\n",
      "       39.1875, 34.5625, 84.125 , 59.0625, 41.9375, 31.625 , 61.8125,\n",
      "       65.8125, 39.625 , 29.8125, 42.8125, 82.4375, 45.1875, 34.5625,\n",
      "       45.5   , 88.9375, 55.0625, 41.75  , 34.75  , 82.    , 65.4375,\n",
      "       43.375 , 32.625 , 56.0625, 73.875 , 41.9375, 31.75  , 35.9375,\n",
      "       80.1875, 44.6875, 33.9375, 32.    , 86.4375, 55.625 , 38.8125,\n",
      "       28.375 , 69.3125, 73.4375, 43.8125, 33.    , 47.0625, 82.5   ,\n",
      "       44.375 , 32.375 , 36.5625, 83.25  , 43.875 , 31.0625, 27.1875,\n",
      "       80.4375, 46.3125, 31.6875, 23.1875, 78.375 , 61.3125, 38.5625,\n",
      "       27.8125, 51.0625, 80.875 , 44.25  , 33.375 , 28.625 , 80.0625,\n",
      "       53.1875, 35.625 , 25.3125, 48.4375, 73.6875, 38.125 , 28.8125,\n",
      "       26.5625, 84.0625, 53.    , 37.375 , 27.0625, 56.8125, 77.5625,\n",
      "       44.0625, 34.    , 29.5   , 81.0625, 54.8125, 37.4375, 27.3125,\n",
      "       43.    , 78.1875, 40.8125, 31.4375]), 'groups': 27}\n",
      "models.deeper_fcn-x1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'models.deeper_fcn-x1'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [8], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ni, name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(modelnames):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(name)\n\u001B[0;32m---> 11\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m preds[name]\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m gi, group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(preds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroups\u001B[39m\u001B[38;5;124m\"\u001B[39m])):\n\u001B[1;32m     13\u001B[0m         yt \u001B[38;5;241m=\u001B[39m y_true[preds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroups\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m group]\n",
      "\u001B[0;31mKeyError\u001B[0m: 'models.deeper_fcn-x1'"
     ]
    }
   ],
   "source": [
    "# calculate MAE and MAPE patient-wise (for training and testing set)\n",
    "dl_results_train = np.zeros((len(modelnames), 1, 2, n_groups_train))\n",
    "dl_results_test = np.zeros((len(modelnames), 1, 2, n_groups_test))\n",
    "\n",
    "for res, preds_ in [(dl_results_train, predictions_dl_train),\n",
    "                    (dl_results_test, predictions_dl_test)]:\n",
    "    y_true, preds = preds_[\"true\"], preds_\n",
    "    print(preds)\n",
    "    for ni, name in enumerate(modelnames):\n",
    "        print(name)\n",
    "        y_pred = preds[name]\n",
    "        for gi, group in enumerate(np.unique(preds[\"groups\"])):\n",
    "            yt = y_true[preds[\"groups\"] == group]\n",
    "            yp = y_pred[preds[\"groups\"] == group]\n",
    "            res[ni, 0, :, gi] = (hr.hr_mape(yt, yp), hr.hr_mae(yt, yp))\n",
    "\n",
    "score_index = 1  # 0: MAPE, 1: MAE\n",
    "rows = []\n",
    "for ni, modelname in enumerate(modelnames):\n",
    "    errmean = dl_results_test.mean(axis=-1)[ni, 0, score_index]\n",
    "    errstd = dl_results_test.std(axis=-1)[ni, 0, score_index]\n",
    "    rows.append((modelname, errmean, errstd))\n",
    "\n",
    "print(\"Results on testing data:\")\n",
    "print(\"========================\\n\")\n",
    "print(tabulate(rows, tablefmt=\"presto\", floatfmt=\".3f\",\n",
    "               headers=[\"modelname\", \"mae\" if score_index==1 else \"mape\", \"std\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}